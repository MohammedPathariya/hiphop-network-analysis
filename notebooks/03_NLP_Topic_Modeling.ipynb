{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c7ed30f-11cc-4acc-b286-b6cab327a858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported and NLTK data downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mohammedpathariya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mohammedpathariya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/mohammedpathariya/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 1: Imports & Setup\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Download NLTK data (run once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Paths\n",
    "PROCESSED_DATA_PATH = '../data/processed'\n",
    "OUTPUT_PATH = '../output'\n",
    "MODELS_PATH = '../output/models'\n",
    "\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "\n",
    "print(\"✅ Libraries imported and NLTK data downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a878c99-2043-4a15-9d20-ec10dfc74f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 700 songs.\n",
      "    artist           title              year  \\\n",
      "0  J. Cole  No Role Modelz  December 9, 2014   \n",
      "1  J. Cole       She Knows  October 29, 2013   \n",
      "\n",
      "                                              lyrics  \n",
      "0  First things first: rest in peace, Uncle Phil\\...  \n",
      "1  She knows\\nShe knows, ayy\\nBad things happen t...  \n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 2: Load Lyrics Data\n",
    "# ==============================================================================\n",
    "lyrics_path = os.path.join(PROCESSED_DATA_PATH, 'song_lyrics.csv')\n",
    "df_lyrics = pd.read_csv(lyrics_path)\n",
    "\n",
    "print(f\"Loaded {len(df_lyrics)} songs.\")\n",
    "print(df_lyrics.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bc03905-2aca-4998-8cda-3d3d971d4529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning lyrics... (this may take a moment)\n",
      "✅ Lyrics cleaned.\n",
      "Example tokens: ['first', 'thing', 'first', 'rest', 'peace', 'uncle', 'phil', 'real', 'father', 'ever']\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 3: Preprocessing Function (The Cleaning Crew)\n",
    "# ==============================================================================\n",
    "# We need to turn raw lyrics into clean lists of words.\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Add custom hip-hop/song stopwords that don't add semantic meaning\n",
    "custom_stops = {'yeah', 'oh', 'like', 'got', 'get', 'know', 'go', 'feat', 'verse', 'chorus', 'intro', 'outro'}\n",
    "stop_words.update(custom_stops)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_lyrics(text):\n",
    "    if not isinstance(text, str): return []\n",
    "    \n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove bracketed text (e.g., [Verse 1], [Chorus])\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    \n",
    "    # 3. Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 4. Tokenize (split into words)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # 5. Remove stopwords and Lemmatize (running -> run)\n",
    "    clean_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    return clean_tokens\n",
    "\n",
    "# Apply cleaning\n",
    "print(\"Cleaning lyrics... (this may take a moment)\")\n",
    "df_lyrics['tokens'] = df_lyrics['lyrics'].apply(clean_lyrics)\n",
    "\n",
    "print(\"✅ Lyrics cleaned.\")\n",
    "print(f\"Example tokens: {df_lyrics['tokens'].iloc[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5c26b1d-266e-4b61-91db-a2c847a24485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary created with 5762 unique tokens.\n",
      "Corpus created for 700 documents.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 4: Prepare for LDA (Dictionary & Corpus)\n",
    "# ==============================================================================\n",
    "# LDA needs a \"Dictionary\" (id -> word mapping) and a \"Corpus\" (word counts per doc)\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(df_lyrics['tokens'])\n",
    "\n",
    "# Filter extremes (remove words that appear in less than 3 songs or more than 50% of songs)\n",
    "# This helps remove rare typos and super-common words like \"rap\"\n",
    "id2word.filter_extremes(no_below=3, no_above=0.5)\n",
    "\n",
    "# Create Corpus\n",
    "corpus = [id2word.doc2bow(text) for text in df_lyrics['tokens']]\n",
    "\n",
    "print(f\"Dictionary created with {len(id2word)} unique tokens.\")\n",
    "print(f\"Corpus created for {len(corpus)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e8c8a73-7c9c-40fd-8b8b-f649a5b05246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA model with 5 topics...\n",
      "✅ Model training complete.\n",
      "Model saved to ../output/models/lda_model\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 5: Train LDA Model\n",
    "# ==============================================================================\n",
    "# This is the AI part. We ask it to find 'k' topics.\n",
    "# We'll start with 5 topics to match our 5 communities, but you can change this number.\n",
    "\n",
    "NUM_TOPICS = 5\n",
    "\n",
    "print(f\"Training LDA model with {NUM_TOPICS} topics...\")\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=NUM_TOPICS, \n",
    "                                       random_state=42,\n",
    "                                       passes=10,\n",
    "                                       workers=2)\n",
    "\n",
    "print(\"✅ Model training complete.\")\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(MODELS_PATH, 'lda_model')\n",
    "lda_model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc651a45-4f45-4a57-8e80-8f69f803d29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Discovered Topics ---\n",
      "Topic 0: 0.019*\"ooh\" + 0.010*\"versace\" + 0.008*\"feel\" + 0.008*\"young\" + 0.007*\"need\" + 0.006*\"dope\" + 0.006*\"bad\" + 0.006*\"light\" + 0.006*\"gon\" + 0.005*\"keep\"\n",
      "\n",
      "Topic 1: 0.010*\"way\" + 0.010*\"woo\" + 0.009*\"money\" + 0.009*\"ayy\" + 0.007*\"woah\" + 0.006*\"lil\" + 0.006*\"hey\" + 0.006*\"gon\" + 0.006*\"gotta\" + 0.005*\"baby\"\n",
      "\n",
      "Topic 2: 0.006*\"think\" + 0.006*\"fuckin\" + 0.004*\"tell\" + 0.004*\"hope\" + 0.004*\"fucking\" + 0.004*\"youre\" + 0.004*\"gon\" + 0.004*\"come\" + 0.004*\"look\" + 0.004*\"give\"\n",
      "\n",
      "Topic 3: 0.009*\"life\" + 0.008*\"right\" + 0.007*\"feel\" + 0.007*\"walk\" + 0.006*\"god\" + 0.006*\"real\" + 0.006*\"talk\" + 0.006*\"man\" + 0.005*\"gang\" + 0.005*\"night\"\n",
      "\n",
      "Topic 4: 0.012*\"girl\" + 0.010*\"wanna\" + 0.009*\"baby\" + 0.008*\"youre\" + 0.008*\"right\" + 0.007*\"tell\" + 0.007*\"need\" + 0.006*\"ill\" + 0.006*\"way\" + 0.006*\"even\"\n",
      "\n",
      "✅ Saved songs with assigned topics to: ../data/processed/song_topics.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 6: View Topics\n",
    "# ==============================================================================\n",
    "print(\"--- Discovered Topics ---\")\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\\n\")\n",
    "    \n",
    "# Assign the dominant topic to each song\n",
    "def get_dominant_topic(bow):\n",
    "    topic_probs = lda_model.get_document_topics(bow)\n",
    "    # Sort by probability and get the top one\n",
    "    topic_probs = sorted(topic_probs, key=lambda x: x[1], reverse=True)\n",
    "    return topic_probs[0][0]\n",
    "\n",
    "df_lyrics['Dominant_Topic'] = [get_dominant_topic(bow) for bow in corpus]\n",
    "\n",
    "# Save the results with topics\n",
    "final_lyrics_path = os.path.join(PROCESSED_DATA_PATH, 'song_topics.csv')\n",
    "df_lyrics.to_csv(final_lyrics_path, index=False)\n",
    "print(f\"✅ Saved songs with assigned topics to: {final_lyrics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15595f75-e8bd-4095-9cc3-e4da2e7512a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
